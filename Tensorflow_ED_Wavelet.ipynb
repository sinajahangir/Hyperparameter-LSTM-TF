{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1HUt6cvzzm_oOuJ9M-jIbGOhGa5ZOSvXU",
      "authorship_tag": "ABX9TyOszF+yvKevcDblUkW4Rvbd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinajahangir/Hyperparameter-LSTM-TF/blob/main/Tensorflow_ED_Wavelet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This Colab notebook showcases the training/deplyment of encoder-decoder (ED) model for multi-step ahead hydrological forecasting.\n",
        "\n",
        "The paper explaining the methodology can be accessed via:\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0022169423002111\n",
        "\n",
        "Bayesian optimization is used for hyperparamter optimization of the ED model\n",
        "\n",
        "The models are trained using pinball loss for probabilistic inference\n",
        "\n",
        "CAMELS data can be obtained at: https://gdex.ucar.edu/dataset/camels.html\n",
        "\n",
        "Reference:\n",
        "A. J. Newman, M. P. Clark, K. Sampson, A. Wood, L. E. Hay, A. Bock, R. J. Viger, D. Blodgett, L. Brekke, J. R. Arnold, T. Hopson, and Q. Duan: Development of a large-sample watershed-scale hydrometeorological dataset for the contiguous USA: dataset characteristics and assessment of regional variability in hydrologic model performance. Hydrol. Earth Syst. Sci., 19, 209-223, doi:10.5194/hess-19-209-2015, 2015."
      ],
      "metadata": {
        "id": "C8nhRwLl9FRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "VmLWTjBl_Bv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize #This is used for Bayesian optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR6TgVsWCQ1P",
        "outputId": "faa12ce9-19cd-4bad-d3ff-900f066979e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (25.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.4\n",
            "    Uninstalling typeguard-4.4.4:\n",
            "      Successfully uninstalled typeguard-4.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZP-q-UEGEJz"
      },
      "source": [
        "\"\"\"\n",
        "Loading necessary libraries\n",
        "\"\"\"\n",
        "#importing necessary libs\n",
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "from os import chdir"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loading necessary libraries\n",
        "\"\"\"\n",
        "from skopt import gp_minimize, forest_minimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt.plots import plot_convergence\n",
        "from skopt.plots import plot_objective, plot_evaluations\n",
        "from skopt.plots import plot_histogram, plot_objective_2D\n",
        "from skopt.utils import use_named_args\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "Csr9vq5BAwhZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loading necessary libraries\n",
        "\"\"\"\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "2ZLKWAzZA0pE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error metrics"
      ],
      "metadata": {
        "id": "wdcqg4RZFojW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSE(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    rmse=(np.nanmean(((Pr-Y)**2)))**0.5\n",
        "    return rmse\n",
        "\n",
        "def MAE(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    error=Y-Pr\n",
        "    return np.nanmean(abs(error))\n",
        "\n",
        "def CC(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    return pearsonr(Pr.flatten(),Y.flatten())[0]\n",
        "\n",
        "def BIAS(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    error=Y-Pr\n",
        "    return np.nanmean(error)\n",
        "\n",
        "def PBIAS(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    error=Y-Pr\n",
        "    return (np.nansum(error)/np.nansum(Y))*100\n",
        "\n",
        "def NSE(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    error=Y-Pr\n",
        "    nse=1-(np.nansum((error)**2))/np.nansum((Y-np.nanmean(Y))**2)\n",
        "    return nse\n",
        "def nRMSE(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    rmse=(np.nanmean(((Pr-Y)**2)))**0.5\n",
        "    nrmse=rmse/np.nanmean(Y)\n",
        "    return nrmse\n",
        "def MAPE(Pr,Y):\n",
        "    Pr=np.reshape(Pr,(-1,1))\n",
        "    Y=np.reshape(Y,(-1,1))\n",
        "    error=(Y-Pr)/(Y)\n",
        "    error[Y==0]=1\n",
        "    mape=np.nansum(np.abs(error))/len(error)\n",
        "    return mape"
      ],
      "metadata": {
        "id": "caENHiW9Fqq3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "dhJLkTX2_QoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to mount Google Drive\n",
        "\n",
        "Assuming data is located in ED_Wavelet (change this)"
      ],
      "metadata": {
        "id": "DB7JPfaZDt-Y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wIY5L1WGJtx"
      },
      "source": [
        "chdir(r'/content/drive/MyDrive/ED_Wavelet') #change this\n",
        "import CamelsFunctions as cf #for pre-processing\n",
        "from sklearn.preprocessing import StandardScaler #normalization/scaling the input/output\n",
        "scaler_x = StandardScaler()\n",
        "scaler_x_dec=StandardScaler() #decoder\n",
        "scaler_y=StandardScaler()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBGUMwRGIyrw"
      },
      "source": [
        "#iid=14154500;09081600;03488000 cathcments of interest\n",
        "iid='14154500'\n",
        "n_steps_in=180 #lookback period\n",
        "n_steps_out=7 # lead time\n",
        "df=pd.read_csv('best_%s_lead%d_data.csv'%(iid,n_steps_out),index_col=0) #read csv file\n",
        "\n",
        "col_names=df.columns #obtain all column names\n",
        "q_dec=[]\n",
        "for ii in col_names:\n",
        "    if ii[0]=='Q':\n",
        "        q_dec.append(ii) #all q related columns\n",
        "\n",
        "df_input=df.drop(labels='Lagged.Q.m3.s', axis=1) #This should be removed\n",
        "df_Q=df_input[['Q.m3.s']]\n",
        "df_dec=df_input[q_dec]\n",
        "df_enc=df_input.iloc[:,:5] #encoder input\n",
        "\n",
        "arr_q=np.asarray(df_Q) #observation\n",
        "data_cat=np.asarray(df_enc)\n",
        "data_cat_dec=np.asarray(df_dec)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK67oSnGIodo"
      },
      "source": [
        "#Training data\n",
        "data_X=data_cat[:int(0.7*len(arr_q))]\n",
        "data_Y=arr_q[:int(0.7*len(arr_q))]\n",
        "data_X_dec=data_cat_dec[:int(0.7*len(arr_q))]\n",
        "  #Transform\n",
        "data_X_tr=scaler_x.fit_transform(data_X)\n",
        "data_X_tr_dec=scaler_x_dec.fit_transform(data_X_dec)\n",
        "data_Y_tr=scaler_y.fit_transform(data_Y)\n",
        "\n",
        "#Validation data\n",
        "data_Xval=data_cat[int(0.7*len(arr_q)):int(0.85*len(arr_q))]\n",
        "data_Xval_dec=data_cat_dec[int(0.7*len(arr_q)):int(0.85*len(arr_q))]\n",
        "data_Yval=arr_q[int(0.7*len(arr_q)):int(0.85*len(arr_q))]\n",
        "  #Transform\n",
        "data_Xval_tr=scaler_x.transform(data_Xval)\n",
        "data_Xval_tr_dec=scaler_x_dec.transform(data_Xval_dec)\n",
        "data_Yval_tr=scaler_y.transform(data_Yval)\n",
        "\n",
        "#Test data\n",
        "data_Xtest=data_cat[int(0.85*len(arr_q)):]\n",
        "data_Xtest_dec=data_cat_dec[int(0.85*len(arr_q)):]\n",
        "data_Ytest=arr_q[int(0.85*len(arr_q)):]\n",
        "\n",
        "  #Transform\n",
        "data_Xtest_tr=scaler_x.transform(data_Xtest)\n",
        "data_Xtest_tr_dec=scaler_x_dec.transform(data_Xtest_dec)\n",
        "data_Ytest_tr=scaler_y.transform(data_Ytest)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wS_MNobJMMx"
      },
      "source": [
        "xtrain,ytrain=cf.split_sequence_multi_train(data_X_tr,data_Y_tr,n_steps_in,n_steps_out,mode='seq')\n",
        "xval,yval=cf.split_sequence_multi_train(data_Xval_tr,data_Yval_tr,n_steps_in,n_steps_out,mode='seq')\n",
        "xtest,ytest=cf.split_sequence_forecast(data_Xtest_tr,data_Ytest_tr,n_steps_in,n_steps_out,mode='seq')\n",
        "\n",
        "xtrain_dec,ytrain_dec=cf.split_sequence_multi_train(data_X_tr_dec,data_Y_tr,n_steps_in,n_steps_out,mode='seq')\n",
        "xval_dec,yval_dec=cf.split_sequence_multi_train(data_Xval_tr_dec,data_Yval_tr,n_steps_in,n_steps_out,mode='seq')\n",
        "xtest_dec,ytest_dec=cf.split_sequence_forecast(data_Xtest_tr_dec,data_Ytest_tr,n_steps_in,n_steps_out,mode='seq')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model development"
      ],
      "metadata": {
        "id": "zSUcruc2A6df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pinball_loss(y_true, y_pred, tau):\n",
        "    error = y_true - y_pred\n",
        "    loss = tf.where(error >= 0, tau * error, (tau - 1) * error)\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "7mXneh20EGRM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OW7sT3-6Z8r"
      },
      "source": [
        "def model_builder_ED(lstm_out,dense_out1=32,dropout1=0.1,activation_dense_1='relu',\\\n",
        "                     nout=n_steps_out,quantile=0.5,n_features=np.shape(xtrain)[-1],n_features_dec=np.shape(xtrain_dec)[-1]):\n",
        "  n_total_features=n_features\n",
        "  n_deterministic_features=n_features_dec\n",
        "  window_len=n_steps_in\n",
        "  latent_dim_lstm=int(lstm_out) # Explicitly cast to integer\n",
        "  latent_dim_dense=int(latent_dim_lstm/2)\n",
        "  forecast_len=n_steps_in\n",
        "# First branch of the net is an lstm which embeds the past\n",
        "  past_inputs = tf.keras.Input(\n",
        "    shape=(window_len, n_total_features), name='past_inputs')\n",
        "# Encoding the past\n",
        "  encoder = tf.keras.layers.LSTM(latent_dim_lstm, return_state=True)\n",
        "  encoder_outputs, state_h, state_c = encoder(past_inputs)\n",
        "\n",
        "  future_inputs = tf.keras.Input(\n",
        "    shape=(forecast_len, n_deterministic_features), name='future_inputs')\n",
        "# Combining future inputs with recurrent branch output\n",
        "  decoder_lstm = tf.keras.layers.LSTM(latent_dim_lstm,recurrent_regularizer=tf.keras.regularizers.l2(0.001),return_sequences=False)\n",
        "  x = decoder_lstm(future_inputs,initial_state=[state_h, state_c])\n",
        "  x = tf.keras.layers.Dense(latent_dim_dense,activation=activation_dense_1)(x)\n",
        "  x = tf.keras.layers.Dropout(dropout1)(x)\n",
        "  output = tf.keras.layers.Dense(n_steps_out, activation='linear')(x)\n",
        "  ED_model = tf.keras.models.Model(inputs=[past_inputs, future_inputs], outputs=output)\n",
        "  ED_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                loss= lambda y_true, y_pred: pinball_loss(y_true, y_pred, tau=quantile),\n",
        "                metrics=[tf.losses.MeanAbsoluteError()])\n",
        "  return ED_model"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example for model structure\n",
        "model_temp=model_builder_ED(lstm_out=128,dense_out1=64)\n",
        "plot_model(model_temp,show_shapes=True,)"
      ],
      "metadata": {
        "id": "0zKKdmIjVjAl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian optimization"
      ],
      "metadata": {
        "id": "cbfrENQAFZQ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU6cS0xh9Ojc"
      },
      "source": [
        "#search space\n",
        "dim_lstm_out = Integer(low=32, high=256, name='lstm_out')\n",
        "dim_dense_out1= Integer(low=16, high=128, name='dense_out1')\n",
        "dim_dropout1= Real(low=0, high=0.3, prior='uniform' ,name='dropout1')\n",
        "dim_dense_active1=Categorical(['relu','tanh','sigmoid'],name='activation_dense_1')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimensions = [dim_lstm_out,dim_dense_out1,dim_dropout1,dim_dense_active1]"
      ],
      "metadata": {
        "id": "zGs4P6qaeeTc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZV2TQjNLDiH"
      },
      "source": [
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(lstm_out,dense_out1,dropout1,activation_dense_1):\n",
        "    \"\"\"\n",
        "    Hyper-parameters:\n",
        "    lstm_out:  Number of lstm outputs (hidden size)\n",
        "    dense_outi:   Number of dense layer outputs\n",
        "    dropouti: Droput rate for dense layer\n",
        "    activation_dense_i: Activation layer for dense\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the hyper-parameters.\n",
        "    print('lstm_out:', lstm_out)\n",
        "    print('dense_out1:', dense_out1)\n",
        "    print('dropout1:', dropout1)\n",
        "    print('activation_dense_1:', activation_dense_1)\n",
        "    print()\n",
        "\n",
        "    # Create the neural network with these hyper-parameters.\n",
        "    model = model_builder_ED(lstm_out=lstm_out,dense_out1=dense_out1,dropout1=dropout1,activation_dense_1=activation_dense_1,\\\n",
        "                     nout=n_steps_out,quantile=0.5,n_features=np.shape(xtrain)[-1],n_features_dec=np.shape(xtrain_dec)[-1])\n",
        "\n",
        "    patience=1 #change this\n",
        "    callback_log = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    restore_best_weights=True,\n",
        "                                                    mode='auto')\n",
        "\n",
        "    # Use Keras to train the model.\n",
        "    history = model.fit(x=[xtrain,xtrain_dec],\n",
        "                        y=ytrain,\n",
        "                        epochs=5, #change this\n",
        "                        verbose=1,\n",
        "                        validation_data=([xval,xval_dec],yval),\n",
        "                        callbacks=[callback_log])\n",
        "\n",
        "    # Get the prediction accuracy on the validation-set\n",
        "    # after the last training-epoch.\n",
        "    predict_model=model.predict(x=[xval,xval_dec]).ravel().reshape((-1,1))\n",
        "    predict_model=scaler_y.inverse_transform(predict_model)\n",
        "    y_validi=yval.ravel().reshape((-1,1))\n",
        "    y_validi=scaler_y.inverse_transform(y_validi)\n",
        "\n",
        "    # We are using NSE to find the best hyperparamters. This can be set\n",
        "    # to any other metric.\n",
        "    accuracy = -NSE(predict_model,y_validi)\n",
        "\n",
        "    # Print the prediction accuracy.\n",
        "    print()\n",
        "    print(\"Val NSE: {0:.4}\".format(-accuracy))\n",
        "    print()\n",
        "\n",
        "    # Save the model if it improves on the best-found performance.\n",
        "    # We use the global keyword so we update the variable outside\n",
        "    # of this function.\n",
        "    global best_accuracy\n",
        "\n",
        "    # If the classification accuracy of the saved model is improved ...\n",
        "    if accuracy < best_accuracy:\n",
        "\n",
        "        # Update the classification accuracy.\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "    # Delete the Keras model with these hyper-parameters from memory.\n",
        "    del model\n",
        "\n",
        "    # Clear the Keras session, otherwise it will keep adding new\n",
        "    # models to the same TensorFlow graph each time we create\n",
        "    # a model with a different set of hyper-parameters.\n",
        "    K.clear_session()\n",
        "\n",
        "    # NOTE: Scikit-optimize does minimization so it tries to\n",
        "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
        "    # Because we are interested in the HIGHEST classification\n",
        "    # accuracy, we need to negate this number so it can be minimized.\n",
        "    return accuracy"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwfmoTSMDeqS"
      },
      "source": [
        "best_accuracy=10\n",
        "search_result = gp_minimize(func=fitness,\n",
        "                            dimensions=dimensions,\n",
        "                            acq_func='EI', # Expected Improvement.\n",
        "                            n_calls=10 ) # change this"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zQl_0JDLieq"
      },
      "source": [
        "# Save results for future use\n",
        "pd_best=pd.DataFrame(search_result.x,index=['lstm_out','dense_out1','dropout1','activation_dense_1'],columns=['parameter'])\n",
        "name_best='best_config_%s_lag_%d_lead_%d_ED_v1.csv'%(iid,n_steps_in,n_steps_out)\n",
        "pd_best.to_csv(name_best)\n",
        "#files.download(name_best)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd7Eh-lsc0ov",
        "outputId": "8c40e693-b61b-4140-d7a5-4ff954a865ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.int64(254), np.int64(111), 0.15781484223674394, 'relu']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrain the model"
      ],
      "metadata": {
        "id": "kmEmHzE8KaZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [0.05, 0.5, 0.95]\n",
        "model_q=[]\n",
        "for q in qs:\n",
        "    model=model_builder_ED(lstm_out=search_result.x[0],dense_out1=search_result.x[1],dropout1=search_result.x[2],activation_dense_1=search_result.x[3],\\\n",
        "                     nout=n_steps_out,quantile=0.5,n_features=np.shape(xtrain)[-1],n_features_dec=np.shape(xtrain_dec)[-1])\n",
        "\n",
        "    model.compile(loss=lambda y,f: tilted_loss(q,y,f), optimizer=Adam(learning_rate=1e-4))\n",
        "    MAX_EPOCHS = 100 #change this\n",
        "    patience=10 #change this\n",
        "    early_stopping_val= tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    restore_best_weights=True,\n",
        "                                                    mode='auto')\n",
        "    learning_rate_decrease=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\\\n",
        "  mode='auto', min_delta=1e-4, cooldown=0, min_lr=1e-6)\n",
        "    print('quantile:%1.2f'%(q))\n",
        "    model.fit(x=[xtrain,xtrain_dec],y=ytrain,epochs=MAX_EPOCHS,\n",
        "                      validation_data=([xval,xval_dec],yval),\n",
        "                      callbacks=[early_stopping_val,learning_rate_decrease])\n",
        "    #model.save('model_%s_lag_%d_lead_%d_quantile_%1.2f_ED.h5'%(iid,n_steps_in,n_steps_out,q))\n",
        "    model_q.append(model) #list of models"
      ],
      "metadata": {
        "id": "s-dTJNYQxhP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}